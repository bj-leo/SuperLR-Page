<ul><li><a class="l" href="#pf9" data-dest-detail='[9,"XYZ",134.765,696.09,null]'>Part I 逻辑回归(LR)简介</a><ul><li><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",134.765,696.09,null]'>Logisitic回归</a><ul><li><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",134.765,526.614,null]'>对二值条件概率建模</a></li><li><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",134.765,568.928,null]'>Logistic回归</a></li></ul></li><li><a class="l" href="#pff" data-dest-detail='[15,"XYZ",134.765,485.843,null]'>Appendices</a></li><li><a class="l" href="#pf11" data-dest-detail='[17,"XYZ",134.765,696.09,null]'>附录</a><ul><li><a class="l" href="#pf11" data-dest-detail='[17,"XYZ",134.765,528.346,null]'>Sigmoid函数, Logistic函数 和 Logit函数</a><ul><li><a class="l" href="#pf11" data-dest-detail='[17,"XYZ",134.765,497.501,null]'>Sigmoid函数</a></li><li><a class="l" href="#pf11" data-dest-detail='[17,"XYZ",134.765,301.061,null]'>Logistic函数</a></li><li><a class="l" href="#pf12" data-dest-detail='[18,"XYZ",134.765,696.09,null]'>Logit函数</a></li></ul></li></ul></li></ul></li><li><a class="l" href="#pf13" data-dest-detail='[19,"XYZ",134.765,696.09,null]'>Part II 从广义线性模型(GLM)出发</a><ul><li><a class="l" href="#pf15" data-dest-detail='[21,"XYZ",134.765,696.09,null]'>广义线性模型</a><ul><li><a class="l" href="#pf15" data-dest-detail='[21,"XYZ",134.765,528.232,null]'>引入广义线性模型</a></li><li><a class="l" href="#pf16" data-dest-detail='[22,"XYZ",134.765,433.701,null]'>深入理解广义线性模型</a></li></ul></li><li><a class="l" href="#pf1b" data-dest-detail='[27,"XYZ",134.765,485.843,null]'>Appendices</a></li><li><a class="l" href="#pf1d" data-dest-detail='[29,"XYZ",134.765,696.09,null]'>附录</a><ul><li><a class="l" href="#pf1d" data-dest-detail='[29,"XYZ",134.765,528.346,null]'>指数分布族补充</a><ul><li><a class="l" href="#pf1d" data-dest-detail='[29,"XYZ",134.765,421.248,null]'>一般形式(GF)</a></li><li><a class="l" href="#pf1d" data-dest-detail='[29,"XYZ",134.765,246.146,null]'>自然形式(NF)</a></li><li><a class="l" href="#pf1e" data-dest-detail='[30,"XYZ",134.765,486.439,null]'>均值自然形式(MVNF)</a></li></ul></li></ul></li></ul></li><li><a class="l" href="#pf1f" data-dest-detail='[31,"XYZ",134.765,696.09,null]'>Part III 结构风险最小(SRM)的视角</a><ul><li><a class="l" href="#pf21" data-dest-detail='[33,"XYZ",134.765,696.09,null]'>结构风险最小(SRM)</a><ul><li><a class="l" href="#pf21" data-dest-detail='[33,"XYZ",134.765,525.729,null]'>什么是经验风险最小(ERM)</a></li><li><a class="l" href="#pf23" data-dest-detail='[35,"XYZ",134.765,482.792,null]'>从经验风险到结构风险</a><ul><li><a class="l" href="#pf25" data-dest-detail='[37,"XYZ",134.765,696.09,null]'>直观地理解SRM(从空间角度)</a></li><li><a class="l" href="#pf26" data-dest-detail='[38,"XYZ",134.765,500.621,null]'>直观地理解SRM(从贝叶斯观点)</a></li><li><a class="l" href="#pf27" data-dest-detail='[39,"XYZ",134.765,532.089,null]'>L2正则化与L1正则化</a></li></ul></li></ul></li><li><a class="l" href="#pf2d" data-dest-detail='[45,"XYZ",134.765,696.09,null]'>正则化(Regularization)</a><ul><li><a class="l" href="#pf2d" data-dest-detail='[45,"XYZ",134.765,461.573,null]'>分类算法的损失函数</a></li><li><a class="l" href="#pf2f" data-dest-detail='[47,"XYZ",134.765,696.09,null]'>分类界限(CM)</a></li><li><a class="l" href="#pf31" data-dest-detail='[49,"XYZ",134.765,487.656,null]'>逻辑回归的过拟合与正则化</a></li></ul></li></ul></li><li><a class="l" href="#pf33" data-dest-detail='[51,"XYZ",134.765,696.09,null]'>Part IV 从两类(Bi-Class)到多类(Multi-Class)</a><ul><li><a class="l" href="#pf35" data-dest-detail='[53,"XYZ",134.765,696.09,null]'>从二项(Binomial)分布到多项(Multinominal)分布</a><ul><li><a class="l" href="#pf35" data-dest-detail='[53,"XYZ",134.765,525.729,null]'>抛硬币(Coin Toss)和掷骰子(Dice)</a><ul><li><a class="l" href="#pf35" data-dest-detail='[53,"XYZ",134.765,495.709,null]'>伯努力(Bernoulli)分布</a></li><li><a class="l" href="#pf35" data-dest-detail='[53,"XYZ",134.765,377.552,null]'>二项(Binomial)分布</a></li><li><a class="l" href="#pf36" data-dest-detail='[54,"XYZ",134.765,696.09,null]'>多项(Multinominal)分布</a></li></ul></li></ul></li><li><a class="l" href="#pf37" data-dest-detail='[55,"XYZ",134.765,696.09,null]'>从二项回归(Binomial Regression)到多项回归(Multinominal Regression)</a><ul><li><a class="l" href="#pf37" data-dest-detail='[55,"XYZ",134.765,525.729,null]'>GLM模型的解释</a></li><li><a class="l" href="#pf38" data-dest-detail='[56,"XYZ",134.765,558.604,null]'>软最大(Softmax)回归</a><ul><li><a class="l" href="#pf38" data-dest-detail='[56,"XYZ",134.765,528.565,null]'>软最大函数</a></li><li><a class="l" href="#pf38" data-dest-detail='[56,"XYZ",134.765,409.393,null]'>软最大回归的解释</a></li></ul></li></ul></li></ul></li><li><a class="l" href="#pf3b" data-dest-detail='[59,"XYZ",134.765,696.09,null]'>Part V 最大熵(MaxEnt)</a><ul><li><a class="l" href="#pf3d" data-dest-detail='[61,"XYZ",134.765,696.09,null]'>最大熵原理和逻辑回归</a><ul><li><a class="l" href="#pf3d" data-dest-detail='[61,"XYZ",134.765,450.949,null]'>最大熵原理(Principle of Maximum Entropy)</a></li><li><a class="l" href="#pf3e" data-dest-detail='[62,"XYZ",134.765,454.71,null]'>最大熵原理与Logistic回归的等价性</a></li><li><a class="l" href="#pf40" data-dest-detail='[64,"XYZ",134.765,653.748,null]'>Log-Linear模型</a></li></ul></li></ul></li><li><a class="l" href="#pf43" data-dest-detail='[67,"XYZ",134.765,696.09,null]'>Part VI 从熵(Entropy)解释GLM, MaxEnt, MLE</a><ul><li><a class="l" href="#pf45" data-dest-detail='[69,"XYZ",134.765,696.09,null]'>来自熵(Entropy)的解释</a><ul><li><a class="l" href="#pf45" data-dest-detail='[69,"XYZ",134.765,461.524,null]'>对信息熵的理解</a><ul><li><a class="l" href="#pf45" data-dest-detail='[69,"XYZ",134.765,245.96,null]'>不确定性公理化解释</a></li><li><a class="l" href="#pf47" data-dest-detail='[71,"XYZ",134.765,218.926,null]'>期望编码长度解释</a></li></ul></li><li><a class="l" href="#pf4b" data-dest-detail='[75,"XYZ",134.765,696.09,null]'>最大信息熵原理</a><ul><li><a class="l" href="#pf4b" data-dest-detail='[75,"XYZ",134.765,668.864,null]'>最大熵解释原理的直观理解</a></li><li><a class="l" href="#pf4c" data-dest-detail='[76,"XYZ",134.765,453.861,null]'>自然指数分布族的最大熵解释</a></li><li><a class="l" href="#pf4e" data-dest-detail='[78,"XYZ",134.765,696.09,null]'>最大似然估计的最大熵解释</a></li></ul></li></ul></li><li><a class="l" href="#pf51" data-dest-detail='[81,"XYZ",134.765,485.843,null]'>Appendices</a></li><li><a class="l" href="#pf53" data-dest-detail='[83,"XYZ",134.765,696.09,null]'>附录</a><ul><li><a class="l" href="#pf53" data-dest-detail='[83,"XYZ",134.765,528.346,null]'>信息熵的由来</a></li></ul></li></ul></li><li><a class="l" href="#pf57" data-dest-detail='[87,"XYZ",134.765,696.09,null]'>Part VII Logistic PCA</a><ul><li><a class="l" href="#pf59" data-dest-detail='[89,"XYZ",134.765,696.09,null]'>Logistic PCA和PCA在指数分布族上的推广</a><ul><li><a class="l" href="#pf59" data-dest-detail='[89,"XYZ",134.765,403.404,null]'>主成份分析(PCA)</a><ul><li><a class="l" href="#pf59" data-dest-detail='[89,"XYZ",134.765,295.646,null]'>最大方差投影</a></li><li><a class="l" href="#pf5a" data-dest-detail='[90,"XYZ",134.765,324.61,null]'>最小重建误差</a></li><li><a class="l" href="#pf5b" data-dest-detail='[91,"XYZ",134.765,330.273,null]'>高斯随机采样</a></li></ul></li><li><a class="l" href="#pf5c" data-dest-detail='[92,"XYZ",134.765,537.088,null]'>PCA在指数分布族上的推广</a><ul><li><a class="l" href="#pf5c" data-dest-detail='[92,"XYZ",134.765,368.112,null]'>自然指数分布族</a></li><li><a class="l" href="#pf5d" data-dest-detail='[93,"XYZ",134.765,281.775,null]'>Bregman散度</a></li><li><a class="l" href="#pf5e" data-dest-detail='[94,"XYZ",134.765,500.372,null]'>PCA在指数分布族上的推广(PCA for the Exponential Family)</a></li></ul></li></ul></li></ul></li></ul>